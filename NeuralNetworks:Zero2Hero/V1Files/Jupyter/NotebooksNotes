=============================================================================================================================================
Video 1: 2:25:51(Length)
The spelled-out intro to neural networks and backpropagation: building micrograd
https://www.youtube.com/watch?v=PaCmpygFfXo
Links:
- micrograd on github: https://github.com/karpathy/micrograd
https://github.com/karpathy/nn-zero-to-hero/tree/master/lectures/micrograd

Notes:
[12] = Scalar
[12, 13] = Vector
[1,2,3]
[12,13,14]=Matrix
[[1,2,3],[4,5,6],[7,8,9]
[12,13,14][15,16,17],[18,19,20]]=Tensor(packaged scalar values, array of arrays/scalers
Micrograd=An autograd(automatic gradient) engine, implements backpropagation
  allows you to build out mathematical expressions, understand tensors at a micro level
backpropagadtion=algorithm that allows you to efficiently evaluate the gradient of some loss function in respect to it's weights of a neural network
  you can iteratively tune the weights to minimize the loss function and improve the network of a the network
Neural networks=Are just mathematical expressions, A neural network is a method in artificial intelligence that teaches computers to process data in a way
  that is inspired by the human brain. It is a type of machine learning process, called deep learning, that uses interconnected nodes or neurons in a layered
  structure that resembles the human brain.
  They take the input data as an input, and the weights as an input as a mathematical expression, and the output is your predictions/loss function
Loss=A single number calculated by a neural net, that somehow measures total performance of NN, you want to minimize the loss
"""
pip install micrograd #install micrograd

#Example usage
#Below is a slightly contrived example showing a number of possible supported operations:

from micrograd.engine import Value

#2 inputs= a,b -4,2 wrapped with Value, a,b are transformed into c,d,e,f,g
a = Value(-4.0)
b = Value(2.0)
c = a + b #add
d = a * b + b**3 #multiply, to the 3rd power
c += c + 1 #offset by 1
c += 1 + c + (-a) #negate
d += d * 2 + (b + a).relu() #squash at 0
d += 3 * d + (b - a).relu()
e = c - d
f = e**2 #squared
g = f / 2.0 #divide by constant, 2
g += 10.0 / f
print(f'{g.data:.4f}') # prints 24.7041, the outcome of this forward pass, g.data gets value of (g)
g.backward() #initializes backpropagation from the node (g), recursively applies chain rule from calculus, evaluates derivatives of (g)
print(f'{a.grad:.4f}') # prints 138.8338, i.e. the numerical value of dg/da, queries derivatives of (g) with respect to (a)
print(f'{b.grad:.4f}') # prints 645.5773, i.e. the numerical value of dg/db, queries derivatives of (g) with respect to (b)
#tells you how (a) and (b) are affecting (g) through mathematical expression
"""
with 2 inputs(a,b), you build out an expression graph, and an output value of (g)
will build out entire mathematical expression, will know (c) is also a value, and maintain pointers for (c) to (a,b)

Video 1 Lecture Time(8:10) Notes for Jupyter Notebooks +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
https://github.com/evilusean/MachineLearning/blob/main/NeuralNetworks:Zero2Hero/V1Files/Jupyter/micrograd_lecture_first_half_roughly.ipynb

#In [11]: Takes in a single value of 'x'
def f(x):
  return 3*x**2 - 4*x + 5 
#In [12]: Returns a Single value 'y'
f(3.0)
#Out [12]: Returns '20.0' as y
#In [13]: 
xs = np.arange(-5, 5, 0.25) #creates an array of x variables from -5 to 5 in steps of 0.25
ys = f(xs) #get a set of ys by calling the above function on the xs
plt.plot(xs, ys) #creates a parabola plot with 2 above xs, and ys variables lists
#Noone working in neural nets actually writes out the expresion used in their neural nets that they learned in calculus, noone derives the derivative
# as limit approaches zero: remember calculus derivative formula 
#In[43]: setting up lim close to 0, Y =m(x) + b  Vs d = a*b + c #remember calculus, this is how he gets the slope
#In[50]: adding (h=0.0001) to (a) and getting a smaller number
#In[55]: Create functions to wrap values and add them together
#In [257]: Starts Creating and explaining Functions at VideoTime(19:00) Finished creating functions for the math used in the notebook
#In:[139]: Videotime(25:00) Copies and pastes code, GraphViz API, allows you to create a graph using 'drawdot'. visualizes forward pass
#In[145]:adds gradient, for seeing impact of weights during back propagation on 'L'=d*f, grad 0=no impact, he manually inputs grads
  #after back propagation, trying to nudge values higher to get a higher gradient, one step of an optimization
#In[136]:Creates a new function for local scope, so as not to mess up global scope. Inline gradient check, trying to find out how 'c' impacts 'L'
  #a '+' node simply passes it's gradient on to it's children; so if it's gradient is -2.00, it's child will be a -2.00
  #chain rule allows you to simply multiply backwards on the chain to get gradient values, back propogation, recursively multiply through chain rule
#In[152]: tanh function from numpy, squashing function, activation function using weights and inputs
#In[241]: graphs a 2D neuron with pointers, takes 2 inputs(x1,x2), weights(w1,w2), bias(b). x1w1 + x2w2 + b, he adds an exponential for hyperbolic tanh funct
#In[199]: for backpropagating a multiplication, multiply the previous gradient(ahead in NN) by the value of the current variable; chain rule
  (w1)weight finds (x1)variable gradient and (x1)variable finds (w1)weight gradient
#VideoTime(1:10:00)Creates functions for automatic backpropagation ._backward() for addition, multiplication
  #need to start at end and work backwards for each variable/weight
#In[235]: creates a sorting function that starts from node o and goes through all children(iterates) and lays them out, then adds self to topo list
Topological Sort: ordering of graphs, such that all the edges go only from left to right, Used for grading all nodes in a neuron
#In[258]/In[259]: Fixed a bug about adding gradients, fixed with +=

Video 1 Lecture Time(1:27:10) Notes for Jupyter Notebooks +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
https://github.com/evilusean/MachineLearning/blob/main/NeuralNetworks:Zero2Hero/V1Files/Jupyter/micrograd_lecture_second_half_roughly.ipynb
#In[268]: fixing bugs, adding functions for wrapping a value(addition), multiplication: fixing null(rmul), adding exponentiation(pow) and division.
#In[320]:broke up tanh to new operations for backward pass,with same result
#In[369]:Import torch #import pytorch, VideoTime(1:39:00) previous functions in micrograd used scalers, pytorch uses tensors(arrays of scalers)
  #o.item produces one item, o.data gives a double, even if only one variable(default tensor has 2 values)
#In[592]:Create class neuron, constructor creates an input with weight(self.w, random number between -1 and 1) bias(self.b)
  #n=Layer(2, 3) creates a set of 3 neurons(layer, list of neurons) 
  #MLP class - takes a number of inputs and creates a list of neurons
  #MLP(3, [4, 4, 1]) #3 input neurons, 2 layers of 4 and 1 output neuron, forward pass
#In[666]:Creates an example data set of 4, with weights, binary classifier neural net, if y's = target, then post x's
#In[682]:First loss function, A single number calculated by a neural net, that somehow measures total performance of NN
  #calculates mean squared error of loss, since it is squared, you will get the absolute number, never a negative, the more off you are, the greater the loss
  #if loss is low, it is close to its target
  #for p in parameters();, slightly decrease gradient to try and decrease loss by += 0.01, forward pass -> backward pass -> update -> repeat
  #if you create too big of a += step, you might overstep and destabilize training
