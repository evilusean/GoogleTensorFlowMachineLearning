Started: 08April2023
End:
PlayList Link:
https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=3
By Andrej Karpathy

=============================================================================================================================================
Video 1: 2:25:51(Length)
The spelled-out intro to neural networks and backpropagation: building micrograd
https://www.youtube.com/watch?v=PaCmpygFfXo
Links:
- micrograd on github: https://github.com/karpathy/micrograd
https://github.com/karpathy/nn-zero-to-hero/tree/master/lectures/micrograd

Notes:
[12] = Scalar
[12, 13] = Vector
[1,2,3]
[12,13,14]=Matrix
[[1,2,3],[4,5,6],[7,8,9]
[12,13,14][15,16,17],[18,19,20]]=Tensor(packaged scalar values, array of arrays/scalers
Micrograd=An autograd(automatic gradient) engine, implements backpropagation
  allows you to build out mathematical expressions, understand tensors at a micro level
backpropagadtion=algorithm that allows you to efficiently evaluate the gradient of some loss function in respect to it's weights of a neural network
  you can iteratively tune the weights to minimize the loss function and improve the network of a the network
Neural networks=Are just mathematical expressions, A neural network is a method in artificial intelligence that teaches computers to process data in a way
  that is inspired by the human brain. It is a type of machine learning process, called deep learning, that uses interconnected nodes or neurons in a layered
  structure that resembles the human brain.
  They take the input data as an input, and the weights as an input as a mathematical expression, and the output is your predictions/loss function
Loss=A single number calculated by a neural net, that somehow measures total performance of NN, you want to minimize the loss
"""
pip install micrograd #install micrograd

#Example usage
#Below is a slightly contrived example showing a number of possible supported operations:

from micrograd.engine import Value

#2 inputs= a,b -4,2 wrapped with Value, a,b are transformed into c,d,e,f,g
a = Value(-4.0)
b = Value(2.0)
c = a + b #add
d = a * b + b**3 #multiply, to the 3rd power
c += c + 1 #offset by 1
c += 1 + c + (-a) #negate
d += d * 2 + (b + a).relu() #squash at 0
d += 3 * d + (b - a).relu()
e = c - d
f = e**2 #squared
g = f / 2.0 #divide by constant, 2
g += 10.0 / f
print(f'{g.data:.4f}') # prints 24.7041, the outcome of this forward pass, g.data gets value of (g)
g.backward() #initializes backpropagation from the node (g), recursively applies chain rule from calculus, evaluates derivatives of (g)
print(f'{a.grad:.4f}') # prints 138.8338, i.e. the numerical value of dg/da, queries derivatives of (g) with respect to (a)
print(f'{b.grad:.4f}') # prints 645.5773, i.e. the numerical value of dg/db, queries derivatives of (g) with respect to (b)
#tells you how (a) and (b) are affecting (g) through mathematical expression
"""
with 2 inputs(a,b), you build out an expression graph, and an output value of (g)
will build out entire mathematical expression, will know (c) is also a value, and maintain pointers for (c) to (a,b)

Video 1 Lecture Time(8:10) Notes for Jupyter Notebooks +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
https://github.com/evilusean/MachineLearning/blob/main/NeuralNetworks:Zero2Hero/V1Files/Jupyter/micrograd_lecture_first_half_roughly.ipynb

#In [11]: Takes in a single value of 'x'
def f(x):
  return 3*x**2 - 4*x + 5 
#In [12]: Returns a Single value 'y'
f(3.0)
#Out [12]: Returns '20.0' as y
#In [13]: 
xs = np.arange(-5, 5, 0.25) #creates an array of x variables from -5 to 5 in steps of 0.25
ys = f(xs) #get a set of ys by calling the above function on the xs
plt.plot(xs, ys) #creates a parabola plot with 2 above xs, and ys variables lists
#Noone working in neural nets actually writes out the expresion used in their neural nets that they learned in calculus, noone derives the derivative
# as limit approaches zero: remember calculus derivative formula 
#In[43]: setting up lim close to 0, Y =m(x) + b  Vs d = a*b + c #remember calculus, this is how he gets the slope
#In[50]: adding (h=0.0001) to (a) and getting a smaller number
#In[55]: Create functions to wrap values and add them together
#In [257]: Starts Creating and explaining Functions at VideoTime(19:00) Finished creating functions for the math used in the notebook
#In:[139]: Videotime(25:00) Copies and pastes code, GraphViz API, allows you to create a graph using 'drawdot'. visualizes forward pass
#In[145]:adds gradient, for seeing impact of weights during back propagation on 'L'=d*f, grad 0=no impact, he manually inputs grads
  #after back propagation, trying to nudge values higher to get a higher gradient, one step of an optimization
#In[136]:Creates a new function for local scope, so as not to mess up global scope. Inline gradient check, trying to find out how 'c' impacts 'L'
  #a '+' node simply passes it's gradient on to it's children; so if it's gradient is -2.00, it's child will be a -2.00
  #chain rule allows you to simply multiply backwards on the chain to get gradient values, back propogation, recursively multiply through chain rule
#In[152]: tanh function from numpy, squashing function, activation function using weights and inputs
#In[241]: graphs a 2D neuron with pointers, takes 2 inputs(x1,x2), weights(w1,w2), bias(b). x1w1 + x2w2 + b, he adds an exponential for hyperbolic tanh funct
#In[199]: for backpropagating a multiplication, multiply the previous gradient(ahead in NN) by the value of the current variable; chain rule
  (w1)weight finds (x1)variable gradient and (x1)variable finds (w1)weight gradient
#VideoTime(1:10:00)Creates functions for automatic backpropagation ._backward() for addition, multiplication
  #need to start at end and work backwards for each variable/weight
#In[235]: creates a sorting function that starts from node o and goes through all children(iterates) and lays them out, then adds self to topo list
Topological Sort: ordering of graphs, such that all the edges go only from left to right, Used for grading all nodes in a neuron
#In[258]/In[259]: Fixed a bug about adding gradients, fixed with +=

Video 1 Lecture Time(1:27:10) Notes for Jupyter Notebooks +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
https://github.com/evilusean/MachineLearning/blob/main/NeuralNetworks:Zero2Hero/V1Files/Jupyter/micrograd_lecture_second_half_roughly.ipynb
#In[268]: fixing bugs, adding functions for wrapping a value(addition), multiplication: fixing null(rmul), adding exponentiation(pow) and division.
#In[320]:broke up tanh to new operations for backward pass,with same result
#In[369]:Import torch #import pytorch, VideoTime(1:39:00) previous functions in micrograd used scalers, pytorch uses tensors(arrays of scalers)
  #o.item produces one item, o.data gives a double, even if only one variable(default tensor has 2 values)
#In[592]:Create class neuron, constructor creates an input with weight(self.w, random number between -1 and 1) bias(self.b)
  #n=Layer(2, 3) creates a set of 3 neurons(layer, list of neurons) 
  #MLP class - takes a number of inputs and creates a list of neurons
  #MLP(3, [4, 4, 1]) #3 input neurons, 2 layers of 4 and 1 output neuron, forward pass
#In[666]:Creates an example data set of 4, with weights, binary classifier neural net, if y's = target, then post x's
#In[682]:First loss function, A single number calculated by a neural net, that somehow measures total performance of NN
  #calculates mean squared error of loss, since it is squared, you will get the absolute number, never a negative, the more off you are, the greater the loss
  #if loss is low, it is close to its target
  #for p in parameters();, slightly decrease gradient to try and decrease loss by += 0.01, forward pass -> backward pass -> update -> repeat
  #if you create too big of a += step, you might overstep and destabilize training
#In[663]: for loop 20 times, forward pass -> backward pass -> update(increase step by a very small amount), print loss output
  #this process is called 'gradient descent' this one is called 'stochastic gradient descent update'
NonLinearities = Sigmoid vs Tanh vs Relu 
VideoTime(2:17:10) Finishes talking about micrograd app and it's functions we just learned during video.
VideoTime(2:18:35) talks about test_engine.py
  Creates 2 chunks of code, one in micrograd and one in pytorch and makes sure they both match
In datasets with millions a random subset is called a 'batch', so you don't forward pass entire dataset

=============================================================================================================================================
Video 2: 1:57:44(Length)
The spelled-out intro to language modeling: building makemore
https://www.youtube.com/watch?v=PaCmpygFfXo&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=2
Links:
- makemore on github: https://github.com/karpathy/makemore
https://github.com/karpathy/nn-zero-to-hero/tree/master/lectures/makemore
Numpy Tutorial: https://cs231n.github.io/python-numpy-tutorial/
Pytorch Tensors: https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html
Pytorch: https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html

MakeMore: makes more of the things you give it; the example used for tutorial was names(training data was 32,000 names).
  Character level- treats every line as an example, with each line representing a sequence of characters, predicting the next character
  each character in a name builds a statistical structure for what is likely to be the next character
Bigram Language Model=Always working with just 2 characters at a time, 
Video 2 Lecture Time(3:07) Notes for Jupyter Notebooks +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
https://github.com/evilusean/MachineLearning/blob/main/NeuralNetworks:Zero2Hero/V2Files/Jupyter/makemore_part1_bigrams.ipynb
In[4]: loads in the words of the .txt data training set, and splits for each line into a python list
In[9]: Creating a bigram 
  b = {} #create a dictionary for counting the amount of times a start and end character occur
  chs = ['<S>'] + list(w) + ['<E>'] #'hallucinate' a special Start token, +a special End character, start character is likely to have 'E'nd character
  for ch1, ch2 in zip(w,w[:1]): # :1 = first name on list(remove for full dataset), for character 1, character 2, in one column, 
  bigram = (ch1, ch2) #creates a bigram of all start characters and end characters
  b.items #returns key value tuple pairs, 
  sorted(b.items(), key = lambda kv: -kv[1]) #sorts key value pairs by the second value of the tuple key value pairs '-' does most likely
It is more efficient to store data in a 2D array instead of python dictionary, torch.tensor allows you to create multi dimensional arrays
Tensors will allow you to manipulate individual entries as well, [indexing]
In[20]/In[365]: create an array 'N' that represents the capitals 28x28 and 2 special characters +26 letters in alphabet
In[366]:set(' '.join(words)) #creates a set(no duplicates) of a single string.join(words) of all the words into a sorted list
  stoi {s:i for i,s in enumerate(chars)} #creates a mapping of each character and enumerates it(gives it a number 0-25(26)
  stoi['<S>'] = 26 stoi['<E>'] = 27 #creates a number for the special characters 'start' and 'end'
In[29]/In[367]: Creates a 28x28 array of all characters(26 letters+2 specials)
In[368]: #use matplotlib to create a pretty visualization of the 28x28 array, color coded based on frequency
  He changed <S> and <E> special characters to '.' for graph visability and removed one special character
In[159]: N[0] takes first row of array
In[160]: converts it into a float probability of distribution instead of an integer count
In[18]:Deterministic generator, seeds a pseudo random function that creates 1 sample for an index of ['m']
In [135]/In[19]:Deterministic generator, seeds a pseudo random function that creates random numbers and normalizes it
In[164]: loop that creates random names, ix grabs index 0, p=N[ix] #while true grabs the row of the index that you are currently on, then normalizes
  generator samples index, if ix == 0; #this is the end token, break out of loop, otherwise print itos(ix) #prints next characters of index
  out[] #creates a list of names created, for i in range(20): #samples 20 names
In[21-28]:p.sum(1).shape #he creates a new array with probability, so you don't need to recalculate for every for loop run
keepdims=True #fixes the division and fixes the division error
In[435]:checks probability maximum liklihood assigned to every bigram in training set
  logprob = torch.log(prob) #(log liklihood: more probability=lower negative log)
  loss function=negative log liklihood, lower it is, is better
  Goal=Maximize liklihood of the data w.r.t model parameters(statistical modeling)
  You can validate the probability for any word= for w in ["sean"]:
  if you try to predict a loss for something that has no probability you get infinity loss function vs Model Smoothing=fake counts= P = N + 1
In[449]: Training Set for bigrams, made up of 2 lists, inputs(x) and targets(y)
  Confusing: there is a lowercase torch.tensor and an Uppercase torch.Tensor only difference is dtype, lowercase automatically sets dtype
In[452]:torch.nn.functional.one_hot used to set weights of certain neurons(xs), encode integers into vectors to feed into neural net
 torch.randn #fills the rest of the torch with random numbers based on a standard deviation distribution pattern 









































