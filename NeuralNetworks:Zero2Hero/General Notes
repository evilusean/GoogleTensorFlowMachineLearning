Started: 08April2023
End:
PlayList Link:
https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=3
By Andrej Karpathy

=============================================================================================================================================
Video 1: 2:25:51(Length)
The spelled-out intro to neural networks and backpropagation: building micrograd
https://www.youtube.com/watch?v=PaCmpygFfXo
Links:
- micrograd on github: https://github.com/karpathy/micrograd
https://github.com/karpathy/nn-zero-to-hero/tree/master/lectures/micrograd

Notes:
[12] = Scalar
[12, 13] = Vector
[1,2,3]
[12,13,14]=Matrix
[[1,2,3],[4,5,6],[7,8,9]
[12,13,14][15,16,17],[18,19,20]]=Tensor(packaged scalar values, array of arrays/scalers
Micrograd=An autograd(automatic gradient) engine, implements backpropagation
  allows you to build out mathematical expressions, understand tensors at a micro level
backpropagadtion=algorithm that allows you to efficiently evaluate the gradient of some loss function in respect to it's weights of a neural network
  you can iteratively tune the weights to minimize the loss function and improve the network of a the network
Neural networks=Are just mathematical expressions, A neural network is a method in artificial intelligence that teaches computers to process data in a way
  that is inspired by the human brain. It is a type of machine learning process, called deep learning, that uses interconnected nodes or neurons in a layered
  structure that resembles the human brain.
  They take the input data as an input, and the weights as an input as a mathematical expression, and the output is your predictions/loss function
Loss=A single number calculated by a neural net, that somehow measures total performance of NN, you want to minimize the loss
"""
pip install micrograd #install micrograd

#Example usage
#Below is a slightly contrived example showing a number of possible supported operations:

from micrograd.engine import Value

#2 inputs= a,b -4,2 wrapped with Value, a,b are transformed into c,d,e,f,g
a = Value(-4.0)
b = Value(2.0)
c = a + b #add
d = a * b + b**3 #multiply, to the 3rd power
c += c + 1 #offset by 1
c += 1 + c + (-a) #negate
d += d * 2 + (b + a).relu() #squash at 0
d += 3 * d + (b - a).relu()
e = c - d
f = e**2 #squared
g = f / 2.0 #divide by constant, 2
g += 10.0 / f
print(f'{g.data:.4f}') # prints 24.7041, the outcome of this forward pass, g.data gets value of (g)
g.backward() #initializes backpropagation from the node (g), recursively applies chain rule from calculus, evaluates derivatives of (g)
print(f'{a.grad:.4f}') # prints 138.8338, i.e. the numerical value of dg/da, queries derivatives of (g) with respect to (a)
print(f'{b.grad:.4f}') # prints 645.5773, i.e. the numerical value of dg/db, queries derivatives of (g) with respect to (b)
#tells you how (a) and (b) are affecting (g) through mathematical expression
"""
with 2 inputs(a,b), you build out an expression graph, and an output value of (g)
will build out entire mathematical expression, will know (c) is also a value, and maintain pointers for (c) to (a,b)

Video 1 Lecture Time(8:10) Notes for Jupyter Notebooks +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
https://github.com/evilusean/MachineLearning/blob/main/NeuralNetworks:Zero2Hero/V1Files/Jupyter/micrograd_lecture_first_half_roughly.ipynb

#In [11]: Takes in a single value of 'x'
def f(x):
  return 3*x**2 - 4*x + 5 
#In [12]: Returns a Single value 'y'
f(3.0)
#Out [12]: Returns '20.0' as y
#In [13]: 
xs = np.arange(-5, 5, 0.25) #creates an array of x variables from -5 to 5 in steps of 0.25
ys = f(xs) #get a set of ys by calling the above function on the xs
plt.plot(xs, ys) #creates a parabola plot with 2 above xs, and ys variables lists
#Noone working in neural nets actually writes out the expresion used in their neural nets that they learned in calculus, noone derives the derivative
# as limit approaches zero: remember calculus derivative formula 
#In[43]: setting up lim close to 0, Y =m(x) + b  Vs d = a*b + c #remember calculus, this is how he gets the slope
#In[50]: adding (h=0.0001) to (a) and getting a smaller number
#In[55]: Create functions to wrap values and add them together
#In [257]: Starts Creating and explaining Functions at VideoTime(19:00) Finished creating functions for the math used in the notebook
#In:[139]: Videotime(25:00) Copies and pastes code, GraphViz API, allows you to create a graph using 'drawdot'. visualizes forward pass
#In[145]:adds gradient, for seeing impact of weights during back propagation on 'L'=d*f, grad 0=no impact, he manually inputs grads
  #after back propagation, trying to nudge values higher to get a higher gradient, one step of an optimization
#In[136]:Creates a new function for local scope, so as not to mess up global scope. Inline gradient check, trying to find out how 'c' impacts 'L'
  #a '+' node simply passes it's gradient on to it's children; so if it's gradient is -2.00, it's child will be a -2.00
  #chain rule allows you to simply multiply backwards on the chain to get gradient values, back propogation, recursively multiply through chain rule
#In[152]: tanh function from numpy, squashing function, activation function using weights and inputs
#In[241]: graphs a 2D neuron with pointers, takes 2 inputs(x1,x2), weights(w1,w2), bias(b). x1w1 + x2w2 + b, he adds an exponential for hyperbolic tanh funct
#In[199]: for backpropagating a multiplication, multiply the previous gradient(ahead in NN) by the value of the current variable; chain rule
  (w1)weight finds (x1)variable gradient and (x1)variable finds (w1)weight gradient
#VideoTime(1:10:00)Creates functions for automatic backpropagation ._backward() for addition, multiplication
  #need to start at end and work backwards for each variable/weight
#In[235]: creates a sorting function that starts from node o and goes through all children(iterates) and lays them out, then adds self to topo list
Topological Sort: ordering of graphs, such that all the edges go only from left to right, Used for grading all nodes in a neuron
#In[258]/In[259]: Fixed a bug about adding gradients, fixed with +=

Video 1 Lecture Time(1:27:10) Notes for Jupyter Notebooks +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
https://github.com/evilusean/MachineLearning/blob/main/NeuralNetworks:Zero2Hero/V1Files/Jupyter/micrograd_lecture_second_half_roughly.ipynb
#In[268]: fixing bugs, adding functions for wrapping a value(addition), multiplication: fixing null(rmul), adding exponentiation(pow) and division.
#In[320]:broke up tanh to new operations for backward pass,with same result
#In[369]:Import torch #import pytorch, VideoTime(1:39:00) previous functions in micrograd used scalers, pytorch uses tensors(arrays of scalers)
  #o.item produces one item, o.data gives a double, even if only one variable(default tensor has 2 values)
#In[592]:Create class neuron, constructor creates an input with weight(self.w, random number between -1 and 1) bias(self.b)
  #n=Layer(2, 3) creates a set of 3 neurons(layer, list of neurons) 
  #MLP class - takes a number of inputs and creates a list of neurons
  #MLP(3, [4, 4, 1]) #3 input neurons, 2 layers of 4 and 1 output neuron, forward pass
#In[666]:Creates an example data set of 4, with weights, binary classifier neural net, if y's = target, then post x's
#In[682]:First loss function, A single number calculated by a neural net, that somehow measures total performance of NN
  #calculates mean squared error of loss, since it is squared, you will get the absolute number, never a negative, the more off you are, the greater the loss
  #if loss is low, it is close to its target
  #for p in parameters();, slightly decrease gradient to try and decrease loss by += 0.01, forward pass -> backward pass -> update -> repeat
  #if you create too big of a += step, you might overstep and destabilize training
#In[663]: for loop 20 times, forward pass -> backward pass -> update(increase step by a very small amount), print loss output
  #this process is called 'gradient descent' this one is called 'stochastic gradient descent update'
NonLinearities = Sigmoid vs Tanh vs Relu 
VideoTime(2:17:10) Finishes talking about micrograd app and it's functions we just learned during video.
VideoTime(2:18:35) talks about test_engine.py
  Creates 2 chunks of code, one in micrograd and one in pytorch and makes sure they both match
In datasets with millions a random subset is called a 'batch', so you don't forward pass entire dataset

=============================================================================================================================================
Video 2: 1:57:44(Length)
The spelled-out intro to language modeling: building makemore
https://www.youtube.com/watch?v=PaCmpygFfXo&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=2
Links:
- makemore on github: https://github.com/karpathy/makemore
https://github.com/karpathy/nn-zero-to-hero/tree/master/lectures/makemore
Numpy Tutorial: https://cs231n.github.io/python-numpy-tutorial/
Pytorch Tensors: https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html
Pytorch: https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html

MakeMore: makes more of the things you give it; the example used for tutorial was names(training data was 32,000 names).
  Character level- treats every line as an example, with each line representing a sequence of characters, predicting the next character
  each character in a name builds a statistical structure for what is likely to be the next character
Bigram Language Model=Always working with just 2 characters at a time, 
Video 2 Lecture Time(3:07) Notes for Jupyter Notebooks +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
https://github.com/evilusean/MachineLearning/blob/main/NeuralNetworks:Zero2Hero/V2Files/Jupyter/makemore_part1_bigrams.ipynb
In[4]: loads in the words of the .txt data training set, and splits for each line into a python list
In[9]: Creating a bigram 
  b = {} #create a dictionary for counting the amount of times a start and end character occur
  chs = ['<S>'] + list(w) + ['<E>'] #'hallucinate' a special Start token, +a special End character, start character is likely to have 'E'nd character
  for ch1, ch2 in zip(w,w[:1]): # :1 = first name on list(remove for full dataset), for character 1, character 2, in one column, 
  bigram = (ch1, ch2) #creates a bigram of all start characters and end characters
  b.items #returns key value tuple pairs, 
  sorted(b.items(), key = lambda kv: -kv[1]) #sorts key value pairs by the second value of the tuple key value pairs '-' does most likely
It is more efficient to store data in a 2D array instead of python dictionary, torch.tensor allows you to create multi dimensional arrays
Tensors will allow you to manipulate individual entries as well, [indexing]
In[20]/In[365]: create an array 'N' that represents the capitals 28x28 and 2 special characters +26 letters in alphabet
In[366]:set(' '.join(words)) #creates a set(no duplicates) of a single string.join(words) of all the words into a sorted list
  stoi {s:i for i,s in enumerate(chars)} #creates a mapping of each character and enumerates it(gives it a number 0-25(26)
  stoi['<S>'] = 26 stoi['<E>'] = 27 #creates a number for the special characters 'start' and 'end'
In[29]/In[367]: Creates a 28x28 array of all characters(26 letters+2 specials)
In[368]: #use matplotlib to create a pretty visualization of the 28x28 array, color coded based on frequency
  He changed <S> and <E> special characters to '.' for graph visability and removed one special character
In[159]: N[0] takes first row of array
In[160]: converts it into a float probability of distribution instead of an integer count
In[18]:Deterministic generator, seeds a pseudo random function that creates 1 sample for an index of ['m']
In [135]/In[19]:Deterministic generator, seeds a pseudo random function that creates random numbers and normalizes it
In[164]: loop that creates random names, ix grabs index 0, p=N[ix] #while true grabs the row of the index that you are currently on, then normalizes
  generator samples index, if ix == 0; #this is the end token, break out of loop, otherwise print itos(ix) #prints next characters of index
  out[] #creates a list of names created, for i in range(20): #samples 20 names
In[21-28]:p.sum(1).shape #he creates a new array with probability, so you don't need to recalculate for every for loop run
keepdims=True #fixes the division and fixes the division error
In[435]:checks probability maximum liklihood assigned to every bigram in training set
  logprob = torch.log(prob) #(log liklihood: more probability=lower negative log)
  loss function=negative log liklihood, lower it is, is better
  Goal=Maximize liklihood of the data w.r.t model parameters(statistical modeling)
  You can validate the probability for any word= for w in ["sean"]:
  if you try to predict a loss for something that has no probability you get infinity loss function vs Model Smoothing=fake counts= P = N + 1
In[449]: Training Set for bigrams, made up of 2 lists, inputs(x) and targets(y)
  Confusing: there is a lowercase torch.tensor and an Uppercase torch.Tensor only difference is dtype, lowercase automatically sets dtype
In[487]:torch.nn.functional.one_hot used to set weights of certain neurons(xs), encode integers into vectors to feed into neural net
In[493]:torch.randn #fills the rest of the torch with random numbers based on a standard deviation distribution pattern (softmax)
In[558]:create probabilities for 5x27 matrix, each row sums to 1, how likely each character is going to come next
In[560]: function that explains the liklihood of each character pair
In[565]/In[566]: training targets for the neural net, will be converted from integers into vectors
In[602]:Forward pass of the neural net of all inputs to get predictions, a single linear layer followed by a softmax
In[604]:loss.backward #initializes backward propagation at the final input node
In[605]:update every single element parameters with += 0.1 to the gradient, nudge to reduce the loss function
In[682]:creates bigram tensor for all words
In[716]:Forward Pass -> Backward Pass -> Update loop x1 on 228146 bigrams

=============================================================================================================================================
Video 3: 1:15:39(Length)
Building makemore Part 2: MLP
https://www.youtube.com/watch?v=TCH_1BHY58I&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=3
MLP(Multi Layer Perceptron)
groups similar terms together on the matrix to try and predict the next word in a sentence. 
Video 3 Lecture Time(9:05) Notes for Jupyter Notebooks +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
https://github.com/evilusean/MachineLearning/blob/main/NeuralNetworks:Zero2Hero/V2Files/Jupyter/makemore_part2_mlp.ipynb
In[4}:Building out the vocabular of character mappings strings to integers and vica versa
In[5]:dataset creation: blocksize #context length of how many characters you are going to use to predict next one, predictions from 3 characters 
  X = inputs to NN, Y = labels for each example in x #x stores current running context, y is the prediction
  crops context showing the 3 characters ----> and predicted character, pads with ... dots
In[15]:C = torch.randn((27,2)) #creating 2D matrix 'C' and seeding with random numbers that is 27x2
In[35]:creates a one_hot vector that sums to 1, and random standard distribution to all elements in tensor
In[126]: creates random weights for 6 inputs, and 100 neurons
.view #changes the way you can view a tensor, dimensions
.storage #tensors are always stored as 1D array,
In[127]: embeds by concatonating and unbinding in case you change the blocksize from 3
In[328]: creates h using tanh on emb.view with weights and bias
In[339]: creates final layer W2= 100 inputs and 27 neurons(characters) with 27 bias
  logits(output) will be h(tanh) * W2(weights) +b2(bias)
  logits are then exponentiated and normalized, 
In[384]/[790]:grouped all of the above code into a single frame
In[386]:Forward pass, takes logits and calculates loss, can use a torch function called cross entropy to get same result
In[523]:added backward pass, and gradient nudge, added for loop 
In[547]:added all words into training data
In[554]: added minibatch construct, in reality while training data, you set a mini batch set of data and train that set instead of a large dataset
In[793]: torch.linspace(min, max, steps) #tries to figure optimal rate to train dataset min amount, max amount, step size
  learning rate decay=at late stages of training, reduce the size of the learning rate
  the larger the dataset, the higher chances you have of overfitting your training data
  to avoid overtraining, split the data x3(training split, development/validation split, test split=80%/10%/10%)
  undertraining is when you train too small of a dataset
In[796]:plots the learning rate
In[710]:visualizes embeddings on neural net
In[797]:uses cross entropy to train instead of forward pass -> backward pass -> update
In[820]: generates 20 samples from the trained model,

=============================================================================================================================================
Video 4: 1:55:57(Length)
Building makemore Part 3: Activations & Gradients, BatchNorm
https://www.youtube.com/watch?v=PaCmpygFfXo&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=2
Links:
- "Kaiming init" paper: https://arxiv.org/abs/1502.01852
- BatchNorm paper: https://arxiv.org/abs/1502.03167
- Bengio et al. 2003 MLP language model paper (pdf): https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf
- Good paper illustrating some of the problems with batchnorm in practice: https://arxiv.org/abs/2105.07576
RNN(Recurring Neural Networks)=not easily optimizable

Video 4 Lecture Time(1:25) Notes for Jupyter Notebooks +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
https://github.com/evilusean/MachineLearning/blob/main/NeuralNetworks:Zero2Hero/V2Files/Jupyter/makemore_part3_bn.ipynb
In[1]:import modules
In[2]:opening words file
In[4]:creating all lower case letters and special '.' 
In[5]: reading in dataset and processing, creating 3 splits; train, dev, and test
In[6]: dimensions of embedding vectors, 200 hidden neurons of MLP, assign values pseudo randomly to weights and biases
In[7]: optimize for over 200k steps, forward pass->backward pass->update
  you can tell initialization is wrong if starting value for loss function is too high, 
  when initializing you want logits to be close to 0 or equal to avoid extreme loss numbers, use softmax
  in the inverted hockey stick, it starts high and flattens out, what is happening is logits are being squashed and rearranged
  tanh is a squashing function
  
In[69]:loss function is negative log probability
In[8]: graph loss function
In[9]: refactored forward pass with split data
  sets standard deviation once, at beginning of training
In[10]: sample from model, forward pass, sample, repeat until special end token,
  also contains batch normalization
In[96]:h.shape #torch.Size([32, 200]) h.view.shape(-1)#stretches out 32 inputs x 200 neurons into one big 6400 vector 
  h.view(-1).tolist()#converts to 1 large list of floats
  plt.hist(h.view(-1).tolist(),50); #creates a histogram of values
Activation Functions=Sigmoid Vs tanh vs ReLU vs Leaky ReLU vs Maxout vs ELU
The deeper a network is; increased complexity, problems stack up, if you have a bad initialization you can get in a situation where you don't train at all
kaiming normal=torch.nn.init vs most common way to initialize a neural net
you can square root any number by exponentiating(**) it to 0.5
Batch Normalization(2015)=made it possible to train deep neural nets very reliably, normalizes hidden states to gaussian distribution
  customary to take linear/convulation layer and append a batch normalization layer right after it,stabilizes training
  'jitter' introduces entropy to training, noise, regularizer, counterintuitively stabilizes training.
  for batch normalization, it is better to not use bias(b1) use bnbias, 
  weight layer -> batch normalization layer -> non-linearity layer(tanh)
  try to avoid batch normalization, if you can, it creates alot of bugs try group normalization or layer normalization instead
  you are trying to avoid loss function going to 0 or infinity with normalization
resnets are used for image classification, an image is fed in, many layers with a repeating structure that attempts to predict what is in image
  convolution layers are linear layers used for images
In[13]:Summary/torchified code/goes over a module structured code:
  class linear #creates a linear layer fan in=inputs, fan_out=outputs, bias= True, with weights
  class BatchNormId #creates batch normalization, kwargs are dimensionality, epsilon, momentum, buffers=running mean, running variance, exponential moving avg
  class Tanh: squashing/loss function
In[14]:200K steps, forward pass(linear)+cross entropy(jitter), backward pass(grad fill), update
In[15]:forward pass activation histogram on tanh layer(softmax)
In[16]:gradient saturation histogram for backward pass optimization
In[17]:updating parameters of NN histogram with weights
In[18]:Plots update ratios over time=updates/data, learning rate of gradients and weights
Last 3 graphs, used to track performance or bottlenecks in neural net
Modules=Layers, can be stacked up in a Neural net like lego building blocks

=============================================================================================================================================
Video 5: 1:55:23(Length)
Building makemore Part 4: Becoming a Backprop Ninja
https://www.youtube.com/watch?v=q8SA3rM6ckI&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=6
Links:
https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b
https://arxiv.org/abs/1502.03167

Video 5 Lecture Time(7:15) Notes for Jupyter Notebooks +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
https://github.com/evilusean/MachineLearning/blob/main/NeuralNetworks:Zero2Hero/V2Files/Jupyter/makemore_part4_backprop.ipynb
In [7]: utility function for gradients
In[8]:Initialization: C=embedding table, first layer=w1, b2, second layer=w2, b2 , batch normalization, 
In[9]: calculating single batch
In[10]: longer forward pass with loss function;logits come out of neural net, find max, subtract max for numerical stability(too large, exponentiatied)
In[210]:Exercise 1:  manual backpropagation without pytorch; dlogprobs=holds derivative of loss with respect to all elements of logprobs[32,27]
  probabilities got by adding up all elements and dividing by mean and applied to .log function
  need to transform the [32,1] column variables into an array of [32,27]
  bn variables are referenced in In[10]:
  all the cmp(...) are manual backpropagation steps
@ = matrix multiply, **=to the power of
keepdim=True #keep dimensions
.clone() #can clone any matrix, safer to work on a copy than the original 
In[211]:simpler way of backpropagating with loss function
In[225]:checks first row 
In[235]:softmax calculates probability, sums to 1
In[239]: Visualizing dlogits=probabilities matrix from forward pass
In[269]:one backward pass for a node then replicated to the other 64 neurons
In[286]:trains with the previous used backward pass, reininitalizes NN, manual backprop, everything that goes on under the hood to give gradients
  each layer is ~3 lines of code
In[285]: Checks gradients

=============================================================================================================================================
Video 6: 56:21(Length)
Building makemore Part 5: Building a WaveNet
https://www.youtube.com/watch?v=t3YJ5hKiMQ0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=8
Links:
https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf
Wavenet2016 by Deepmind:
https://arxiv.org/abs/1609.03499

Video 6 Lecture Time(1:40) Notes for Jupyter Notebooks +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
https://github.com/evilusean/MachineLearning/blob/main/NeuralNetworks:Zero2Hero/V2Files/Jupyter/makemore_part5_cnn1.ipynb
In[1]:Imports
In[2]:reading dataset
In[3]:builds sets of characters
In[4]:shuffle words
In[5]:processing set of words into examples of 3 chars trying to predict 4th
In[7]:API's similar to pytorch created manually, for each layer; linear, batch normalization, tanh layers
In[8]:seed random data 
In[9]:creates embedding layers, and parameters
In[10]:optimization/training for 200 000 steps, forward pass -> backward pass -> update
In[11]: plots loss function
In[12]:evaluation for training mode =  False
In[13]:evaluates the loss function
In[14]:samples the model




