=============================================================================================================================================
Video 2: 1:57:44(Length)
The spelled-out intro to language modeling: building makemore
https://www.youtube.com/watch?v=PaCmpygFfXo&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=2
Links:
- makemore on github: https://github.com/karpathy/makemore
https://github.com/karpathy/nn-zero-to-hero/tree/master/lectures/makemore
Numpy Tutorial: https://cs231n.github.io/python-numpy-tutorial/
Pytorch Tensors: https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html
Pytorch: https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html

MakeMore: makes more of the things you give it; the example used for tutorial was names(training data was 32,000 names).
  Character level- treats every line as an example, with each line representing a sequence of characters, predicting the next character
  each character in a name builds a statistical structure for what is likely to be the next character
Bigram Language Model=Always working with just 2 characters at a time, 
Video 2 Lecture Time(3:07) Notes for Jupyter Notebooks +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
https://github.com/evilusean/MachineLearning/blob/main/NeuralNetworks:Zero2Hero/V2Files/Jupyter/makemore_part1_bigrams.ipynb
In[4]: loads in the words of the .txt data training set, and splits for each line into a python list
In[9]: Creating a bigram 
  b = {} #create a dictionary for counting the amount of times a start and end character occur
  chs = ['<S>'] + list(w) + ['<E>'] #'hallucinate' a special Start token, +a special End character, start character is likely to have 'E'nd character
  for ch1, ch2 in zip(w,w[:1]): # :1 = first name on list(remove for full dataset), for character 1, character 2, in one column, 
  bigram = (ch1, ch2) #creates a bigram of all start characters and end characters
  b.items #returns key value tuple pairs, 
  sorted(b.items(), key = lambda kv: -kv[1]) #sorts key value pairs by the second value of the tuple key value pairs '-' does most likely
It is more efficient to store data in a 2D array instead of python dictionary, torch.tensor allows you to create multi dimensional arrays
Tensors will allow you to manipulate individual entries as well, [indexing]
In[20]/In[365]: create an array 'N' that represents the capitals 28x28 and 2 special characters +26 letters in alphabet
In[366]:set(' '.join(words)) #creates a set(no duplicates) of a single string.join(words) of all the words into a sorted list
  stoi {s:i for i,s in enumerate(chars)} #creates a mapping of each character and enumerates it(gives it a number 0-25(26)
  stoi['<S>'] = 26 stoi['<E>'] = 27 #creates a number for the special characters 'start' and 'end'
In[29]/In[367]: Creates a 28x28 array of all characters(26 letters+2 specials)
In[368]: #use matplotlib to create a pretty visualization of the 28x28 array, color coded based on frequency
  He changed <S> and <E> special characters to '.' for graph visability and removed one special character
In[159]: N[0] takes first row of array
In[160]: converts it into a float probability of distribution instead of an integer count
In[18]:Deterministic generator, seeds a pseudo random function that creates 1 sample for an index of ['m']
In [135]/In[19]:Deterministic generator, seeds a pseudo random function that creates random numbers and normalizes it
In[164]: loop that creates random names, ix grabs index 0, p=N[ix] #while true grabs the row of the index that you are currently on, then normalizes
  generator samples index, if ix == 0; #this is the end token, break out of loop, otherwise print itos(ix) #prints next characters of index
  out[] #creates a list of names created, for i in range(20): #samples 20 names
In[21-28]:p.sum(1).shape #he creates a new array with probability, so you don't need to recalculate for every for loop run
keepdims=True #fixes the division and fixes the division error
In[435]:checks probability maximum liklihood assigned to every bigram in training set
  logprob = torch.log(prob) #(log liklihood: more probability=lower negative log)
  loss function=negative log liklihood, lower it is, is better
  Goal=Maximize liklihood of the data w.r.t model parameters(statistical modeling)
  You can validate the probability for any word= for w in ["sean"]:
  if you try to predict a loss for something that has no probability you get infinity loss function vs Model Smoothing=fake counts= P = N + 1
In[449]: Training Set for bigrams, made up of 2 lists, inputs(x) and targets(y)
  Confusing: there is a lowercase torch.tensor and an Uppercase torch.Tensor only difference is dtype, lowercase automatically sets dtype
In[487]:torch.nn.functional.one_hot used to set weights of certain neurons(xs), encode integers into vectors to feed into neural net
In[493]:torch.randn #fills the rest of the torch with random numbers based on a standard deviation distribution pattern (softmax)
In[558]:create probabilities for 5x27 matrix, each row sums to 1, how likely each character is going to come next
In[560]: function that explains the liklihood of each character pair
In[565]/In[566]: training targets for the neural net, will be converted from integers into vectors
In[602]:Forward pass of the neural net of all inputs to get predictions, a single linear layer followed by a softmax
In[604]:loss.backward #initializes backward propagation at the final input node
In[605]:update every single element parameters with += 0.1 to the gradient, nudge to reduce the loss function
In[682]:creates bigram tensor for all words
In[716]:Forward Pass -> Backward Pass -> Update loop x1 on 228146 bigrams

=============================================================================================================================================
Video 3: 1:15:39(Length)
Building makemore Part 2: MLP
https://www.youtube.com/watch?v=TCH_1BHY58I&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=3
MLP(Multi Layer Perceptron)
groups similar terms together on the matrix to try and predict the next word in a sentence. 
Video 3 Lecture Time(9:05) Notes for Jupyter Notebooks +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
https://github.com/evilusean/MachineLearning/blob/main/NeuralNetworks:Zero2Hero/V2Files/Jupyter/makemore_part2_mlp.ipynb
In[4}:Building out the vocabular of character mappings strings to integers and vica versa
In[5]:dataset creation: blocksize #context length of how many characters you are going to use to predict next one, predictions from 3 characters 
  X = inputs to NN, Y = labels for each example in x #x stores current running context, y is the prediction
  crops context showing the 3 characters ----> and predicted character, pads with ... dots
In[15]:C = torch.randn((27,2)) #creating 2D matrix 'C' and seeding with random numbers that is 27x2
In[35]:creates a one_hot vector that sums to 1, and random standard distribution to all elements in tensor
In[126]: creates random weights for 6 inputs, and 100 neurons
.view #changes the way you can view a tensor, dimensions
.storage #tensors are always stored as 1D array,
In[127]: embeds by concatonating and unbinding in case you change the blocksize from 3
In[328]: creates h using tanh on emb.view with weights and bias
In[339]: creates final layer W2= 100 inputs and 27 neurons(characters) with 27 bias
  logits(output) will be h(tanh) * W2(weights) +b2(bias)
  logits are then exponentiated and normalized, 
In[384]/[790]:grouped all of the above code into a single frame
In[386]:Forward pass, takes logits and calculates loss, can use a torch function called cross entropy to get same result
In[523]:added backward pass, and gradient nudge, added for loop 
In[547]:added all words into training data
In[554]: added minibatch construct, in reality while training data, you set a mini batch set of data and train that set instead of a large dataset
In[793]: torch.linspace(min, max, steps) #tries to figure optimal rate to train dataset min amount, max amount, step size
  learning rate decay=at late stages of training, reduce the size of the learning rate
  the larger the dataset, the higher chances you have of overfitting your training data
  to avoid overtraining, split the data x3(training split, development/validation split, test split=80%/10%/10%)
  undertraining is when you train too small of a dataset
In[796]:plots the learning rate
In[710]:visualizes embeddings on neural net
In[797]:uses cross entropy to train instead of forward pass -> backward pass -> update
In[820]: generates 20 samples from the trained model,
